{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compact-registrar",
   "metadata": {},
   "source": [
    "Credit : https://github.com/nicolashernandez/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "undefined-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "finished-sharing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ahamid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "economic-gossip",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maaf</td>\n",
       "      <td>La MAAF est une assurance qui me parait cher a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eca Assurances</td>\n",
       "      <td>Une tromperie de haut niveau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alptis</td>\n",
       "      <td>J'ai été adhérent pendant des années, très con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Néoliane Santé</td>\n",
       "      <td>Très bonne mutuelle 48 heures les retransmissi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mgen</td>\n",
       "      <td>Depuis longtemps à la MGEN, dégradation des va...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name                                            Comment\n",
       "0             Maaf  La MAAF est une assurance qui me parait cher a...\n",
       "1   Eca Assurances                       Une tromperie de haut niveau\n",
       "2           Alptis  J'ai été adhérent pendant des années, très con...\n",
       "3  Néoliane Santé   Très bonne mutuelle 48 heures les retransmissi...\n",
       "4             Mgen  Depuis longtemps à la MGEN, dégradation des va..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/ahamid/Downloads/archive/Comments.csv', encoding='utf-8')\n",
    "df = df[['Name','Comment']]\n",
    "df = df.sample(100)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-jordan",
   "metadata": {},
   "source": [
    "### Clean words from special chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fatal-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tokens(string, maxsplit=0):\n",
    "    # Tokenize the URL\n",
    "    delimiters = \"-\", \"/\", \".\", \":\", \"_\", \"+\", \"=\", \"&\", \"*\", \",\", \"?\", \"!\", \"$\", \"(\", \")\", \"[\", \"]\", \"@\", \"^\", \"\\\\\", \"'\", \" \"\n",
    "    regex_pattern = '|'.join(map(re.escape, delimiters))\n",
    "    tokens = re.split(regex_pattern, string, maxsplit)\n",
    "    filtered_tokens = filter(None, tokens)\n",
    "    list_of_tokens = list(filtered_tokens)\n",
    "    tokens = ' '.join(list_of_tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "affected-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_subject = df.apply(lambda row: get_word_tokens(row['Comment']), axis=1)\n",
    "df.insert(len(df.columns), 'tokenized_Comment', tokenized_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "unlimited-sector",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <td>Néoliane Santé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Comment</th>\n",
       "      <td>Très bonne mutuelle 48 heures les retransmissi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenized_Comment</th>\n",
       "      <td>Très bonne mutuelle 48 heures les retransmissi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   3\n",
       "Name                                                 Néoliane Santé \n",
       "Comment            Très bonne mutuelle 48 heures les retransmissi...\n",
       "tokenized_Comment  Très bonne mutuelle 48 heures les retransmissi..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df.loc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-budapest",
   "metadata": {},
   "source": [
    "### nltk snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dependent-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = list(df['tokenized_Comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-locking",
   "metadata": {},
   "source": [
    "### exemple Besoin de text preprocessing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "returning-orbit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Très bonne mutuelle 48 heures les retransmissions \r\n",
      "j y suis depuis 2 ans rien à signaler service gestion à l écoute à chaque demande merci à vous\n"
     ]
    }
   ],
   "source": [
    "print(subs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "covered-daniel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject d'origine : Très bonne mutuelle 48 heures les retransmissions...\r\n",
      "j'y suis depuis 2 ans rien à signaler service gestion à l'écoute à chaque demande. merci à vous.\n",
      "\n",
      "subject cleaned : Très bonne mutuelle 48 heures les retransmissions \r\n",
      "j y suis depuis 2 ans rien à signaler service gestion à l écoute à chaque demande merci à vous\n",
      "\n",
      "subject stemmed : ['tres', 'bon', 'mutuel', '48', 'heur', 'le', 'retransm', 'j', 'y', 'suis', 'depuis', '2', 'an', 'rien', 'à', 'signal', 'servic', 'gestion', 'à', 'l', 'écout', 'à', 'chaqu', 'demand', 'merc', 'à', 'vous']\n"
     ]
    }
   ],
   "source": [
    "print(\"subject d'origine :\", df['Comment'][3])\n",
    "print(\"\\nsubject cleaned :\", subs[3])\n",
    "print(\"\\nsubject stemmed :\", [FrenchStemmer().stem(w) for w in word_tokenize(subs[3], language='french')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-thailand",
   "metadata": {},
   "source": [
    "### spacy ????????????????????<br>\n",
    "**Install spacy based on yout conf**\n",
    "https://spacy.io/usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "linear-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download fr_core_news_sm \n",
    "# !python -m spacy download fr_dep_news_trf \n",
    "# !pip install spacy-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "clear-tackle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "spoken-soldier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject d'origine : Très bonne mutuelle 48 heures les retransmissions...\r\n",
      "j'y suis depuis 2 ans rien à signaler service gestion à l'écoute à chaque demande. merci à vous.\n",
      "\n",
      "subject cleaned : Très bonne mutuelle 48 heures les retransmissions \r\n",
      "j y suis depuis 2 ans rien à signaler service gestion à l écoute à chaque demande merci à vous\n",
      "\n",
      "subject stemmed : ['tres', 'bon', 'mutuel', '48', 'heur', 'le', 'retransm', 'j', 'y', 'suis', 'depuis', '2', 'an', 'rien', 'à', 'signal', 'servic', 'gestion', 'à', 'l', 'écout', 'à', 'chaqu', 'demand', 'merc', 'à', 'vous']\n",
      "\n",
      "subject lemmatized : ['très', 'bon', 'mutuelle', '48', 'heure', 'le', 'retransmission', '\\r\\n', 'j', 'y', 'être', 'depuis', '2', 'an', 'rien', 'à', 'signaler', 'service', 'gestion', 'à', 'l', 'écoute', 'à', 'chaque', 'demande', 'merci', 'à', 'vous']\n"
     ]
    }
   ],
   "source": [
    "print(\"subject d'origine :\", df['Comment'][3])\n",
    "print(\"\\nsubject cleaned :\", subs[3])\n",
    "print(\"\\nsubject stemmed :\", [FrenchStemmer().stem(w) for w in word_tokenize(subs[3], language='french')])\n",
    "print(\"\\nsubject lemmatized :\", [token.lemma_ for token in nlp(subs[3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "celtic-timber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['très', 'bon', 'mutuelle', '48', 'heure', 'le', 'retransmission', '\\r\\n', 'j', 'y', 'suivre', 'depuis', '2', 'an', 'rien', 'à', 'signaler', 'service', 'gestion', 'à', 'l', 'écoute', 'à', 'chaque', 'demande', 'merci', 'à', 'vous']\n"
     ]
    }
   ],
   "source": [
    "# spacy lemmatization\n",
    "import spacy_transformers\n",
    "nlp = spacy.load('fr_dep_news_trf') # accuracy : CELUI là marche mieux contextuellement et le premier est plus fonctionnel\n",
    "print([token.lemma_ for token in nlp(subs[3])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-pulse",
   "metadata": {},
   "source": [
    "### FrenchLefffLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "manual-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "trained-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "\n",
    "french_lefff_lemmatizer = FrenchLefffLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "neutral-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_to_lefff_pos = {\n",
    "    \"ADJ\": \"adj\",\n",
    "    \"ADP\": \"det\",\n",
    "    \"ADV\": \"adv\",\n",
    "    \"DET\": \"det\",\n",
    "    \"PRON\": \"cln\",\n",
    "    \"PROPN\": \"np\",\n",
    "    \"NOUN\": \"nc\",\n",
    "    \"VERB\": \"v\",\n",
    "    \"PUNCT\": \"poncts\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "beginning-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def french_lefff_lemmatizer_context_sensitive_wi_spacy_pos (french_lefff_lemmatizer, spacy_doc):\n",
    "    ''' \n",
    "    Given that lefff returns wrong lemma for DET and PRON\n",
    "    Given that spacy returns wrong lemma for VERB\n",
    "    Returns the spacy lemma except for VERB\n",
    "    '''\n",
    "    lemmas = []\n",
    "    for t in spacy_doc:\n",
    "        print ([t.text,t.pos_,t.lemma_, spacy_to_lefff_pos[t.pos_] if t.pos_ in spacy_to_lefff_pos else t.pos_, french_lefff_lemmatizer.lemmatize(t.text.lower()), french_lefff_lemmatizer.lemmatize(t.text,spacy_to_lefff_pos[t.pos_] if t.pos_ in spacy_to_lefff_pos else '')])\n",
    "        if t.pos_ in ['VERB']:\n",
    "            print ('to fix')\n",
    "            lefff_lemma = french_lefff_lemmatizer.lemmatize(t.text, spacy_to_lefff_pos[t.pos_])\n",
    "            if type(lefff_lemma) != type (\"\") and len(lefff_lemma) !=0:\n",
    "                lefff_lemma = lefff_lemma[0][0]\n",
    "                print ('is a list so lemma is : '+lefff_lemma)\n",
    "            else: \n",
    "                lefff_lemma =lefff_lemma\n",
    "                print ('is a string so lemma is : '+lefff_lemma)\n",
    "            lemmas.append(lefff_lemma)\n",
    "        else:\n",
    "            lemmas.append(t.lemma_)\n",
    "\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "several-junior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Très bonne mutuelle 48 heures les retransmissions \\r\\nj y suis depuis 2 ans rien à signaler service gestion à l écoute à chaque demande merci à vous'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = subs[3]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "requested-coordinate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Très', 'ADV', 'très', 'adv', 'très', [('très', 'adv')]]\n",
      "['bonne', 'ADJ', 'bon', 'adj', 'bonne', [('bon', 'adj')]]\n",
      "['mutuelle', 'NOUN', 'mutuelle', 'nc', 'mutuelle', [('mutuelle', 'nc')]]\n",
      "['48', 'NUM', '48', 'NUM', '48', []]\n",
      "['heures', 'NOUN', 'heure', 'nc', 'heure', [('heure', 'nc')]]\n",
      "['les', 'DET', 'le', 'det', 'les', [('le', 'det')]]\n",
      "['retransmissions', 'NOUN', 'retransmission', 'nc', 'retransmission', [('retransmission', 'nc')]]\n",
      "['\\r\\n', 'SPACE', '\\r\\n', 'SPACE', '\\r\\n', []]\n",
      "['j', 'PRON', 'j', 'cln', 'j', []]\n",
      "['y', 'PRON', 'y', 'cln', 'y', []]\n",
      "['suis', 'VERB', 'suivre', 'v', 'suis', 'être']\n",
      "to fix\n",
      "is a string so lemma is : être\n",
      "['depuis', 'ADP', 'depuis', 'det', 'depuis', []]\n",
      "['2', 'NUM', '2', 'NUM', '2', []]\n",
      "['ans', 'NOUN', 'an', 'nc', 'an', [('an', 'nc')]]\n",
      "['rien', 'PRON', 'rien', 'cln', 'rien', []]\n",
      "['à', 'ADP', 'à', 'det', 'à', []]\n",
      "['signaler', 'VERB', 'signaler', 'v', 'signaler', 'signaler']\n",
      "to fix\n",
      "is a string so lemma is : signaler\n",
      "['service', 'NOUN', 'service', 'nc', 'service', [('service', 'nc')]]\n",
      "['gestion', 'NOUN', 'gestion', 'nc', 'gestion', [('gestion', 'nc')]]\n",
      "['à', 'ADP', 'à', 'det', 'à', []]\n",
      "['l', 'DET', 'l', 'det', 'l', []]\n",
      "['écoute', 'NOUN', 'écoute', 'nc', 'écoute', [('écoute', 'nc')]]\n",
      "['à', 'ADP', 'à', 'det', 'à', []]\n",
      "['chaque', 'DET', 'chaque', 'det', 'chaque', [('chaque', 'det')]]\n",
      "['demande', 'NOUN', 'demande', 'nc', 'demande', [('demande', 'nc')]]\n",
      "['merci', 'NOUN', 'merci', 'nc', 'merci', [('merci', 'nc')]]\n",
      "['à', 'ADP', 'à', 'det', 'à', []]\n",
      "['vous', 'PRON', 'vous', 'cln', 'vous', [('il', 'cln')]]\n",
      "['très', 'bon', 'mutuelle', '48', 'heure', 'le', 'retransmission', '\\r\\n', 'j', 'y', 'être', 'depuis', '2', 'an', 'rien', 'à', 'signaler', 'service', 'gestion', 'à', 'l', 'écoute', 'à', 'chaque', 'demande', 'merci', 'à', 'vous']\n"
     ]
    }
   ],
   "source": [
    "print(french_lefff_lemmatizer_context_sensitive_wi_spacy_pos(french_lefff_lemmatizer, nlp(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-crowd",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-borough",
   "metadata": {},
   "source": [
    "## spacy-lefff\n",
    "**Custom French POS and lemmatizer based on Lefff for spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "deluxe-local",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy-lefff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "manual-hotel",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple NOUN NPP apple NOUN apple\n",
      "cherche NOUN V chercher NOUN cherche\n",
      "a AUX V avoir AUX avoir\n",
      "acheter VERB VINF acheter VERB acheter\n",
      "une DET DET un DET un\n",
      "startup NOUN NC startup NOUN startup\n",
      "anglaise ADJ ADJ anglais ADJ anglais\n",
      "pour ADP P pour ADP pour\n",
      "1 NUM DET 1 NUM 1\n",
      "milliard NOUN NC milliard NOUN milliard\n",
      "de ADP P de ADP de\n",
      "dollard PROPN NC dollard PROPN dollard\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy_lefff import LefffLemmatizer, POSTagger\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.factory('french_lemmatizer')\n",
    "def create_french_lemmatizer(nlp, name):\n",
    "    return LefffLemmatizer(after_melt=True, default=True)\n",
    "\n",
    "@Language.factory('melt_tagger')  \n",
    "def create_melt_tagger(nlp, name):\n",
    "    return POSTagger()\n",
    " \n",
    "nlp = spacy.load('fr_core_news_sm') # efficiency\n",
    "\n",
    "nlp.add_pipe('melt_tagger', after='parser')\n",
    "nlp.add_pipe('french_lemmatizer', after='melt_tagger')\n",
    "doc = nlp(u\"Apple cherche a acheter une startup anglaise pour 1 milliard de dollard\")\n",
    "for d in doc:\n",
    "    print(d.text, d.pos_, d._.melt_tagger, d._.lefff_lemma, d.tag_, d.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-delta",
   "metadata": {},
   "source": [
    "# stanza aka Neural Stanford corenlp\n",
    "Aside from the neural pipeline, this package also includes an official wrapper for accessing the Java Stanford CoreNLP software with Python code https://github.com/stanfordnlp/CoreNLP.\n",
    "\n",
    "800 Mo of parameters + 600 for the French model (stored in ~/stanza_resources/fr/default.zip)\n",
    "\n",
    "https://github.com/stanfordnlp/stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "inside-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "intense-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "stanza.download('fr')       # This downloads the English models for the neural pipeline\n",
    "nlp = stanza.Pipeline('fr') # This sets up a default neural pipeline in English\n",
    "# doc = nlp(\"Barack Obama was born in Hawaii.  He was elected president in 2008.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "doc.sentences[0].print_dependencies()\n",
    "\n",
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        print(word.text, word.lemma, word.pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-nation",
   "metadata": {},
   "source": [
    "Correct lemmatization on VERBs ; incorrect lemmatization for PRONouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-florence",
   "metadata": {},
   "source": [
    "# treetagger-python\n",
    "A Python module for interfacing with the Treetagger by Helmut Schmid\n",
    "\n",
    "Wrapper source https://github.com/miotto/treetagger-python (alternative exists)\n",
    "License GPL-v3\n",
    "TreeTagger source https://www.cis.lmu.de/~schmid/tools/TreeTagger/\n",
    "A neural version exists too. It lemmatizes all tokens. Lemmas of unknown tokens are guessed and are therefore not guaranteed to be always correct. Slower, requires PyTorch, requires a GPU for improved speed, larger parameter files https://www.cis.lmu.de/~schmid/tools/RNNTagger/ (3.1 GB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-relaxation",
   "metadata": {},
   "source": [
    "## CLTK, The Classical Language Toolkit\n",
    "The Classical Language Toolkit (CLTK) is a Python library offering natural language processing (NLP) for pre-modern languages.\n",
    "\n",
    "Home https://github.com/cltk/cltk\n",
    "Licence MIT\n",
    "Doc https://docs.cltk.org/en/latest/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "therapeutic-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user cltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "accepting-korea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk import NLP\n",
    "\n",
    "#  Middle French language\n",
    "cltk_nlp = NLP(language=\"mfr\")\n",
    "cltk_doc = cltk_nlp.analyze(text=text)\n",
    "print(cltk_doc.lemmata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
